# 네이버 블로그 데이터 크롤링 서비스

## 개요
이 프로젝트는 네이버 블로그 데이터 크롤링을 통해 다양한 메타데이터와 콘텐츠를 추출하는 시스템입니다.
Naver Search API와 Selenium을 활용하여 블로그 데이터를 수집하고, 이를 구조화된 형태로 제공합니다.

---

## 주요 기능
네이버 블로그 데이터를 크롤링하고 주요 키워드와 OCR로 필터링 진행

---

## 프로젝트 구조

(사진 첨부 예정)

### 폴더 및 파일(함수) 설명

- /utils : 크롤링과 필터링 관련 모듈
    - /crawler : 블로그 콘텐츠 데이터, 블로그 메타 데이터, 블로거 메타 데이터를 크롤링, functions 폴더의 함수들 사용
    - /extractor : 가짜 리뷰는 광고성/후원성 키워드를, 진짜 리뷰는 '내돈내산' 키워드를 기준으로 filter 폴더의 함수들을 이용해 각 데이터 추출
    - /filter : OCR, url, 텍스트 기반 필터링
    - /functions : crawler 폴더에서 크롤링에 사용할 미니 함수들 포함한 폴더
- /main : 프로젝트의 시작 지점으로, 크롬 드라이버를 초기화 및 API 인증 정보 로드를 진행하고 장소 키워드 리스트를 기반으로 맛집 리뷰 크롤링 진행

### 주요 흐름
main.py → crawler 폴더의 각 크롤러 순차적으로 실행 → crawler 내부에서 functions의 미니 함수들 호츨

---

## 설치 및 실행 방법
1. 필요한 라이브러리 설치
   - pip install -r requirements.txt
2. 필요한 파일 업로드
    - .env 파일에 네이버 API 인증 정보(CLIENT_ID, CLIENT_ID) 저장
3. main.py 파일 실행

---

## 수정 예정 사항
- 글 업로드 주기, 하루 평균 업로드 개수, 업로드 날짜 크롤링 함수 개발
- 포스트 메타 데이터의 포스트 제목 길이, 이미지 저장 경로, 이미지 + 이모지 개수 크롤러는 임시로 blog_content에서 크롤링 (모듈 역할에 맞게 코드 리팩토링 필요)
- search.py 함수에서 진행하는 코드가 main.py에 있는데 나중에 모듈화 필요함 (지금은 search.py 함수 확인 불필요)